{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMZ1RtmrOsn",
        "outputId": "a85c7823-8cc3-4373-d62e-544ddc887712"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --single-branch --branch fast_tokenizers_BARTpho_PhoBERT_BERTweet https://github.com/datquocnguyen/transformers.git\n",
        "!cd transformers\n",
        "!pip install -e "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYuC8Fh-qaWW",
        "outputId": "c466fadb-6ac1-4520-abf3-b83bca6efb17"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "-e option requires 1 argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUOr4ubs8D1r",
        "outputId": "d7f01696-703f-455c-ed86-c4fbb76bcb16"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (0.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import pipeline,AutoTokenizer,AutoModel,MBartForConditionalGeneration,MBartModel,BartModel,BartphoTokenizer,BartForConditionalGeneration,MBartTokenizerFast,MBartTokenizer,BartTokenizer,MBartForCausalLM"
      ],
      "metadata": {
        "id": "fo8mJLbvvN5W"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "OhJW_yPIqYf8"
      },
      "outputs": [],
      "source": [
        "#BARTpho-syllable\n",
        "syllable_tokenizer = transformers.AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n",
        "bartpho_syllable = transformers.AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
        "TXT=\"Chúng_tôi là những nghiên_cứu_viên .\"  \n",
        "input_ids = syllable_tokenizer(TXT, return_tensors='pt').input_ids\n",
        "features = bartpho_syllable(input_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BARTpho-word\n",
        "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word-base\")\n",
        "word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word-base\")\n",
        "TXT =\"Chúng_tôi là những nghiên_cứu_viên.\" \n",
        "input_ids = word_tokenizer(TXT,return_tensors=\"pt\").input_ids\n",
        "features = bartpho_word(input_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2VhFOyRrqLy",
        "outputId": "3e5a483e-b262-42a5-9275-6a5fad87f90e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    }
  ]
}